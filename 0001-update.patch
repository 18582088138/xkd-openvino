From ceaf50913d1c57dfba3c4477478c7f08eb7385bd Mon Sep 17 00:00:00 2001
From: fishbell <bell.song@intel.com>
Date: Tue, 21 May 2024 21:46:23 +0800
Subject: [PATCH] update

---
 .../src/dev/threading/cpu_message.cpp         |  4 ++-
 .../intel_gpu/plugin/sync_infer_request.hpp   |  8 ++++-
 .../src/plugin/async_infer_request.cpp        |  2 ++
 .../src/plugin/sync_infer_request.cpp         | 36 ++++++++++++++++---
 4 files changed, 44 insertions(+), 6 deletions(-)

diff --git a/src/inference/src/dev/threading/cpu_message.cpp b/src/inference/src/dev/threading/cpu_message.cpp
index 8d6f19c445..1afadceb79 100644
--- a/src/inference/src/dev/threading/cpu_message.cpp
+++ b/src/inference/src/dev/threading/cpu_message.cpp
@@ -186,7 +186,9 @@ public:
 
 std::shared_ptr<MessageManage> message_manager() {
     static MessageManageHolder message_manage;
-    return message_manage.get();
+    auto a = message_manage.get();
+    std::cout << "message: " << message_manage.get() << std::endl;
+    return a;
 }
 
 }  // namespace threading
diff --git a/src/plugins/intel_gpu/include/intel_gpu/plugin/sync_infer_request.hpp b/src/plugins/intel_gpu/include/intel_gpu/plugin/sync_infer_request.hpp
index cdead8a816..f2e7399d45 100644
--- a/src/plugins/intel_gpu/include/intel_gpu/plugin/sync_infer_request.hpp
+++ b/src/plugins/intel_gpu/include/intel_gpu/plugin/sync_infer_request.hpp
@@ -8,7 +8,7 @@
 #include "openvino/runtime/isync_infer_request.hpp"
 #include "intel_gpu/plugin/graph.hpp"
 #include "intel_gpu/plugin/remote_tensor.hpp"
-
+// #include "intel_gpu/plugin/async_infer_request.hpp"
 #include <string>
 #include <map>
 #include <vector>
@@ -19,6 +19,7 @@ namespace ov {
 namespace intel_gpu {
 
 class CompiledModel;
+class AsyncInferRequest;
 
 enum class TensorOwner : uint8_t {
     USER = 0,
@@ -66,6 +67,10 @@ public:
 
     bool use_external_queue() const { return m_use_external_queue; }
 
+    void set_async_request(AsyncInferRequest* asyncRequest);
+
+    void sub_streams_infer();
+
 private:
     void check_tensors() const override;
 
@@ -82,6 +87,7 @@ private:
 
     std::map<cldnn::primitive_id, cldnn::network_output> m_internal_outputs;
     VariablesMap m_variables;
+    AsyncInferRequest* m_asyncRequest = nullptr;
 
     std::shared_ptr<Graph> m_graph;
     RemoteContextImpl::Ptr m_context = nullptr;
diff --git a/src/plugins/intel_gpu/src/plugin/async_infer_request.cpp b/src/plugins/intel_gpu/src/plugin/async_infer_request.cpp
index 0d29114eaa..5545e9f0c6 100644
--- a/src/plugins/intel_gpu/src/plugin/async_infer_request.cpp
+++ b/src/plugins/intel_gpu/src/plugin/async_infer_request.cpp
@@ -25,6 +25,8 @@ AsyncInferRequest::AsyncInferRequest(const std::shared_ptr<SyncInferRequest>& in
                             m_infer_request->wait_notify();
                         });
     }
+    // static_cast<SyncInferRequest*>(infer_request.get())->set_async_request(this);
+    m_infer_request->set_async_request(this);
 }
 void AsyncInferRequest::start_async() {
     if (m_infer_request->use_external_queue()) {
diff --git a/src/plugins/intel_gpu/src/plugin/sync_infer_request.cpp b/src/plugins/intel_gpu/src/plugin/sync_infer_request.cpp
index 8b61470c7b..3386634a50 100644
--- a/src/plugins/intel_gpu/src/plugin/sync_infer_request.cpp
+++ b/src/plugins/intel_gpu/src/plugin/sync_infer_request.cpp
@@ -20,6 +20,8 @@
 #include "intel_gpu/runtime/internal_properties.hpp"
 #include "intel_gpu/runtime/itt.hpp"
 #include "intel_gpu/runtime/debug_configuration.hpp"
+#include "intel_gpu/plugin/async_infer_request.hpp"
+#include "openvino/runtime/threading/cpu_message.hpp"
 
 #include <algorithm>
 #include <iterator>
@@ -113,10 +115,22 @@ SyncInferRequest::SyncInferRequest(const std::shared_ptr<const CompiledModel>& c
 
 void SyncInferRequest::infer() {
     OV_ITT_SCOPED_TASK(itt::domains::intel_gpu_plugin, "SyncInferRequest::infer");
-
-    // if (m_asyncRequest->m_sub_infers) {
-    //     std::cout << "m_asyncRequest->m_sub_infers\n";
-    // }
+    auto message = ov::threading::message_manager();
+    if (m_asyncRequest->m_sub_infers) {
+        std::cout << "m_asyncRequest->m_sub_infers\n";
+        message->server_wait(message->getSubInferRequest().size());
+        std::cout << "message->getSubInferRequest().size: " << message->getSubInferRequest().size() << std::endl;
+        ov::threading::MessageInfo msg_info;
+        msg_info.msg_type = ov::threading::MsgType::START_INFER;
+        ov::threading::Task task = [&] {
+            SyncInferRequest::sub_streams_infer();
+        };
+        msg_info.task = std::move(task);
+        message->send_message(msg_info);
+        message->infer_wait();
+        // std::cout << "------ infer end -----\n";
+        return;
+    }
 
     setup_stream_graph();
     std::lock_guard<std::mutex> lk(m_graph->get_mutex());
@@ -124,6 +138,15 @@ void SyncInferRequest::infer() {
     wait();
 }
 
+void SyncInferRequest::sub_streams_infer() {
+    // sub streams infer
+    auto message = ov::threading::message_manager();
+    auto requests = message->getSubInferRequest();
+    std::cout << "getSubInferRequest\n";
+    // auto inputs = get_inputs();
+    // auto outputs = get_outputs();
+}
+
 std::vector<ov::ProfilingInfo> SyncInferRequest::get_profiling_info() const {
     OPENVINO_ASSERT(m_enable_profiling, "[GPU] Profiling data was not collected: please check that ov::enable_profiling property was set to true");
     return m_graph->get_profiling_info();
@@ -137,6 +160,11 @@ std::vector<ov::SoPtr<ov::IVariableState>> SyncInferRequest::query_state() const
     return ret;
 }
 
+void SyncInferRequest::set_async_request(ov::intel_gpu::AsyncInferRequest* asyncRequest) {
+    m_asyncRequest = asyncRequest;
+    // m_asyncRequest->
+}
+
 void SyncInferRequest::set_tensor(const ov::Output<const ov::Node>& port, const ov::SoPtr<ov::ITensor>& tensor) {
     OV_ITT_SCOPED_TASK(itt::domains::intel_gpu_plugin, "SyncInferRequest::set_tensor");
     const auto& port_info = find_port(port);
-- 
2.34.1

